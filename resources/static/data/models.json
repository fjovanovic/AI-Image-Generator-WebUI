{
    "stabilityai/stable-diffusion-xl-base-1.0": {
        "link": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",
        "info": "[SDXL](https://arxiv.org/abs/2307.01952) consists of an [ensemble of experts](https://arxiv.org/abs/2211.01324) pipeline for latent diffusion: In a first step, the base model is used to generate (noisy) latents, which are then further processed with a refinement model (available [here](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/)) specialized for the final denoising steps. Note that the base model can be used as a standalone module.\n\nAlternatively, we can use a two-stage pipeline as follows: First, the base model is used to generate latents of the desired output size. In the second step, we use a specialized high-resolution model and apply a technique called SDEdit (check on [Arxiv](https://arxiv.org/abs/2108.01073), also known as `img2img`) to the latents generated in the first step, using the same prompt. This technique is slightly slower than the first one, as it requires more function evaluations.\n\nSource code is available on [Github](https://github.com/Stability-AI/generative-models)\n\nWhen using `torch >= 2.0`, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline: `pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)`\n\nIf you are limited by GPU VRAM, you can enable cpu offloading by calling: `pipe.enable_model_cpu_offload` instead of `to(\"cuda\")`\n\nFor more information on how to use Stable Diffusion XL with `diffusers`, please have a look at [the Stable Diffusion XL Docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl)\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model"
    },
    "stabilityai/sdxl-turbo": {
        "link": "https://huggingface.co/stabilityai/sdxl-turbo",
        "info": "SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation. A real-time demo is available [here](http://clipdrop.co/stable-diffusion-turbo). \n\nSDXL-Turbo is a distilled version of [SDXL 1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), trained for real-time synthesis. SDXL-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the [technical report](https://stability.ai/research/adversarial-diffusion-distillation)), which allows sampling large-scale foundational image diffusion models in 1 to 4 steps at high image quality. This approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.\n\nPreferably, the model generates images of size `512x512` but higher image sizes work as well"
    },
    "CompVis/stable-diffusion-v1-1": {
        "link": "https://huggingface.co/CompVis/stable-diffusion-v1-1",
        "info": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at [Stable Diffusion with Diffusers blog](https://huggingface.co/blog/stable_diffusion).\n\n**The Stable-Diffusion-v1-1** was trained on 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/relaion2B-en-research-safe), followed by 194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`). For more information, please refer to [Training](https://huggingface.co/CompVis/stable-diffusion-v1-1#training).\n\nThis weights here are intended to be used with the Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, [come here](https://huggingface.co/CompVis/stable-diffusion-v-1-1-original).\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are primarily limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\nCheckpoints:\n   - <u>stable-diffusion-v1-1</u>: 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/relaion2B-en-research-safe). 194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`)."
    },
    "CompVis/stable-diffusion-v1-2": {
        "link": "https://huggingface.co/CompVis/stable-diffusion-v1-2",
        "info": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at [Stable Diffusion with Diffusers blog](https://huggingface.co/blog/stable_diffusion).\n\n**The Stable-Diffusion-v1-2** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-1](https://huggingface.co/CompVis/stable-diffusion-v1-1) checkpoint and subsequently fine-tuned on 515,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" (a subset of laion2B-en, filtered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. For more information, please refer to [Training](https://huggingface.co/CompVis/stable-diffusion-v1-2#training).\n\nThis weights here are intended to be used with the Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, [come here](https://huggingface.co/CompVis/stable-diffusion-v-1-2-original).\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are primarily limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\nCheckpoints:\n   - <u>stable-diffusion-v1-2</u>: Resumed from `stable-diffusion-v1-1`. 515,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" (a subset of laion2B-en, filtered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor))."
    },
    "CompVis/stable-diffusion-v1-3": {
        "link": "https://huggingface.co/CompVis/stable-diffusion-v1-3",
        "info": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at [Stable Diffusion with Diffusers blog](https://huggingface.co/blog/stable_diffusion).\n\n**The Stable-Diffusion-v1-3** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https://huggingface.co/steps/huggingface.co/CompVis/stable-diffusion-v1-2) checkpoint and subsequently fine-tuned on 195,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598). For more information, please refer to [Training](https://huggingface.co/CompVis/stable-diffusion-v1-3#training).\n\nThis weights here are intended to be used with the Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, [come here](https://huggingface.co/CompVis/stable-diffusion-v-1-3-original)\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are primarily limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\nCheckpoints:\n   - <u>stable-diffusion-v1-3</u>: Resumed from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)."
    },
    "CompVis/stable-diffusion-v1-4": {
        "link": "https://huggingface.co/CompVis/stable-diffusion-v1-4",
        "info": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at [Stable Diffusion with Diffusers blog](https://huggingface.co/blog/stable_diffusion).\n\n**The Stable-Diffusion-v1-4** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https://huggingface.co/steps/huggingface.co/CompVis/stable-diffusion-v1-2) checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.\n\nThis weights here are intended to be used with the Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, [come here](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original).\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are primarily limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\nThe intended use of this model is with the [Safety Checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) in Diffusers. This checker works by checking model outputs against known hard-coded NSFW concepts. The concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter. Specifically, the checker compares the class probability of harmful concepts in the embedding space of the `CLIPTextModel` _after generation_ of the images. The concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\n\nCheckpoints:\n   - <u>stable-diffusion-v1-4</u>: Resumed from `stable-diffusion-v1-2`.225,000 steps at resolution `512x512` on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)."
    },
    "runwayml/stable-diffusion-v1-5": {
        "link": "https://huggingface.co",
        "info": "Missing"
    }
}